{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "    z = np.clip(z, -500, 500)  # Limit z to avoid overflow\n",
    "    return np.where(z >= 0,\n",
    "                    1 / (1 + np.exp(-z)),\n",
    "                    np.exp(z) / (1 + np.exp(z)))\n",
    "\n",
    "def cross_entropy(y, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def compute_gradients(X, y, y_pred):\n",
    "    diff = y_pred - y\n",
    "    return np.dot(X.T, diff) / len(y)\n",
    "\n",
    "def initialize_weights(size):\n",
    "    std_dev = np.sqrt(2 / (size + 1))\n",
    "    return np.random.randn(size) * std_dev\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.weights = initialize_weights(X_train.shape[1])\n",
    "        self.losses = []\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = sigmoid(np.dot(X_train, self.weights))\n",
    "            loss = cross_entropy(y_train, y_pred)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            gradients = compute_gradients(X_train, y_train, y_pred)\n",
    "            self.weights -= self.lr * gradients\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        return sigmoid(np.dot(X, self.weights))\n",
    "        \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_probabilities(X) >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "class KNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = np.bincount(k_nearest_labels).argmax()\n",
    "        return most_common\n",
    "    \n",
    "    def predict_probabilities(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        probs = np.bincount(k_nearest_labels, minlength=np.max(self.y_train)+1) / self.k\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INCARCARE SETURI DE DATE DE ANTRENARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/match_data.csv')\n",
    "X_train_1 = df1.drop(columns=['Date', 'Team1', 'Team2', 'Score'])\n",
    "y_train_1 = df1['Score']\n",
    "ds1_name = \"match_data\"\n",
    "\n",
    "df2 = pd.read_csv('../data/cleaned_rounds_data.csv')\n",
    "df2 = df2.drop(columns=[f'player_{i}_{suffix}' for i in range(1, 11) for suffix in ['team_name', 'name']])\n",
    "X_train_2 = df2.drop(['round_winner'], axis=1)\n",
    "y_train_2 = df2['round_winner']\n",
    "ds2_name = \"rounds_data\"\n",
    "\n",
    "df3 = pd.read_csv('../data/cleaned_rounds_data_with_stats.csv')\n",
    "df3 = df3.drop(columns=[f'player_{i}_{suffix}' for i in range(1, 11) for suffix in ['team_name', 'name']])\n",
    "X_train_3 = df3.drop(columns=['round_winner'])\n",
    "y_train_3 = df3['round_winner']\n",
    "ds3_name = \"rounds_data_with_stats\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETRII OPTIMI AI FIECARUI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate, epochs\n",
    "best_params_logistic_regression = [[0.1, 500], [0.3, 2000], [0.1, 600]]\n",
    "\n",
    "# learning_rate, dropout_rate, l2_lambda, batch_size, epochs\n",
    "best_params_neural_network = [[0.001, 0.5, 0.0001, 64, 500], [0.0005, 0.3, 0.001, 64, 1000], [0.0001, 0.3, 0.01, 128, 300]]\n",
    "\n",
    "# max_depth, learning_rate, gamma, reg_lambda, n_estimators\n",
    "best_params_xgboost = [[3, 0.3, 0, 0.2, 7], [9, 0.05, 0.2, 0.2, 146], [9, 0.1, 0.3, 0, 62]]\n",
    "\n",
    "# learning_rate, max_depth, n_estimators\n",
    "best_params_adaboost = [[0.05, 2, 150], [0.1, 9, 22], [0.05, 10, 52]]\n",
    "\n",
    "# k\n",
    "best_params_knn = [[5], [45], [40]]\n",
    "\n",
    "# max_depth, n_estimators, min_samples_split\n",
    "best_params_random_forest = [[9, 300, 5], [40, 150, 3], [25, 300, 7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INCARCARE SETURI DE DATE DE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('demo_data/test_10_matches_data.csv')\n",
    "X_test_1 = df1.drop(columns=['Date', 'Team1', 'Team2', 'Score'])\n",
    "y_test_1 = df1['Score']\n",
    "ds1_name = \"match_data\"\n",
    "\n",
    "df2 = pd.read_csv('demo_data/test_10_rounds_data.csv')\n",
    "df2 = df2.drop(columns=[f'player_{i}_{suffix}' for i in range(1, 11) for suffix in ['team_name', 'name']])\n",
    "X_test_2 = df2.drop(['round_winner'], axis=1)\n",
    "y_test_2 = df2['round_winner']\n",
    "ds2_name = \"rounds_data\"\n",
    "\n",
    "df3 = pd.read_csv('demo_data/test_10_rounds_data_with_stats.csv')\n",
    "df3 = df3.drop(columns=[f'player_{i}_{suffix}' for i in range(1, 11) for suffix in ['team_name', 'name']])\n",
    "X_test_3 = df3.drop(columns=['round_winner'])\n",
    "y_test_3 = df3['round_winner']\n",
    "ds3_name = \"rounds_data_with_stats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [(X_train_1, y_train_1, X_test_1, y_test_1, ds1_name), (X_train_2, y_train_2, X_test_2, y_test_2, ds2_name), (X_train_3, y_train_3, X_test_3, y_test_3, ds3_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintMessages(idx, y_test):\n",
    "    if idx == 0:\n",
    "        print(\"Actual Match Winner\")\n",
    "    else:\n",
    "        print(\"Actual Round Winner\")\n",
    "    print(y_test.values)\n",
    "\n",
    "    if idx == 0:\n",
    "        print(\"Predicting Match Winner\")\n",
    "    else:\n",
    "        print(\"Predicting Round Winner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION TRAIN AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegressionModel(X_train, y_train, X_test, y_test, learning_rate, epochs):\n",
    "        start_time = time()\n",
    "        model = LogisticRegression(lr=learning_rate, epochs=epochs)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        duration = time() - start_time\n",
    "        print(f\"{y_pred.astype(int)} predicted with Logistic Regression Duration: {duration:.2f}s Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK TRAIN AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetworkModel(X_train, y_train, X_test, y_test, learning_rate, dropout_rate, l2_lambda, batch_size, epochs):\n",
    "        start_time = time()\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X_train.shape[1],)))\n",
    "        model.add(Dense(128, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs, callbacks=[es], verbose=0)\n",
    "        \n",
    "        y_probs = model.predict(X_test)\n",
    "        y_pred = (y_probs > 0.5)\n",
    "\n",
    "        duration = time() - start_time\n",
    "        print(f\"{y_pred.astype(int).ravel()} predicted with Neural Network Duration: {duration:.2f}s Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST TRAIN AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoostModel(X_train, y_train, X_test, y_test, max_depth, learning_rate, gamma, reg_lambda, n_estimators):\n",
    "        start_time = time()\n",
    "        model = XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, gamma=gamma, reg_lambda=reg_lambda, n_estimators=n_estimators)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        duration = time() - start_time\n",
    "        print(f\"{y_pred} predicted with XGBoost Duration: {duration:.2f}s Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABOOST TRAIN AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoostModel(X_train, y_train, X_test, y_test, learning_rate, max_depth, n_estimators):\n",
    "        start_time = time()\n",
    "        model = AdaBoostClassifier(algorithm=\"SAMME\", estimator = DecisionTreeClassifier(max_depth=max_depth), learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        duration = time() - start_time\n",
    "        print(f\"{y_pred} predicted with AdaBoost Duration: {duration:.2f}s Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN TRAIN AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNModel(X_train, y_train, X_test, y_test, k):\n",
    "        start_time = time()\n",
    "        model = KNN(k=k)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = [model.predict(x) for x in X_test]\n",
    "        duration = time() - start_time\n",
    "        y_pred_str = ' '.join(map(str, y_pred))\n",
    "        print(f\"[{y_pred_str}] predicted with KNN Duration: {duration:.2f}s Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST TRAIN AND PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestModel(X_train, y_train, X_test, y_test, max_depth, n_estimators, min_samples_split):\n",
    "        start_time = time()\n",
    "        model = RandomForestClassifier(criterion=\"log_loss\", n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        duration = time() - start_time\n",
    "        print(f\"{y_pred} predicted with RandomForest Duration: {duration:.2f}s Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: match_data\n",
      "Actual Match Winner\n",
      "[1 1 1 1 1 1 0 0 1 1]\n",
      "Predicting Match Winner\n",
      "[0 0 1 0 1 1 0 1 1 1] predicted with Logistic Regression Duration: 0.38s Accuracy: 0.60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "[0 1 1 0 1 1 0 1 1 1] predicted with Neural Network Duration: 2.28s Accuracy: 0.70\n",
      "[0 0 1 0 1 0 0 1 0 1] predicted with XGBoost Duration: 0.01s Accuracy: 0.40\n",
      "[0 0 0 0 1 0 0 1 0 1] predicted with AdaBoost Duration: 0.83s Accuracy: 0.30\n",
      "[1 1 0 0 1 1 0 1 1 1] predicted with KNN Duration: 0.11s Accuracy: 0.70\n",
      "[0 1 1 0 1 1 0 1 1 1] predicted with RandomForest Duration: 0.44s Accuracy: 0.70\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset: rounds_data\n",
      "Actual Round Winner\n",
      "[1 1 0 0 1 0 0 1 1 0]\n",
      "Predicting Round Winner\n",
      "[1 1 1 0 0 0 0 1 1 1] predicted with Logistic Regression Duration: 7.87s Accuracy: 0.70\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "[1 1 0 0 0 0 0 1 1 0] predicted with Neural Network Duration: 11.59s Accuracy: 0.90\n",
      "[1 1 0 0 0 1 0 0 1 0] predicted with XGBoost Duration: 1.21s Accuracy: 0.70\n",
      "[1 1 0 0 0 0 0 1 1 0] predicted with AdaBoost Duration: 7.17s Accuracy: 0.90\n",
      "[1 1 0 0 0 1 1 1 1 1] predicted with KNN Duration: 0.73s Accuracy: 0.60\n",
      "[1 1 0 0 0 0 0 1 1 0] predicted with RandomForest Duration: 0.56s Accuracy: 0.90\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset: rounds_data_with_stats\n",
      "Actual Round Winner\n",
      "[1 0 1 0 0 0 1 0 0 1]\n",
      "Predicting Round Winner\n",
      "[1 0 0 0 0 0 1 1 1 0] predicted with Logistic Regression Duration: 2.66s Accuracy: 0.60\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "[1 0 0 0 0 0 1 1 1 0] predicted with Neural Network Duration: 12.86s Accuracy: 0.60\n",
      "[1 0 0 0 0 0 1 1 0 0] predicted with XGBoost Duration: 0.94s Accuracy: 0.70\n",
      "[1 0 0 0 0 0 1 0 1 1] predicted with AdaBoost Duration: 31.52s Accuracy: 0.80\n",
      "[1 0 0 0 1 0 1 1 0 0] predicted with KNN Duration: 0.81s Accuracy: 0.60\n",
      "[1 0 0 0 0 0 1 1 0 1] predicted with RandomForest Duration: 1.41s Accuracy: 0.80\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (X_train, y_train, X_test, y_test, ds_name) in enumerate(datasets):\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "    print(f\"Dataset: {ds_name}\")\n",
    "\n",
    "    PrintMessages(idx, y_test)\n",
    "\n",
    "    LogisticRegressionModel(X_train, y_train, X_test, y_test, learning_rate=best_params_logistic_regression[idx][0], epochs=best_params_logistic_regression[idx][1])\n",
    "\n",
    "    NeuralNetworkModel(X_train, y_train, X_test, y_test, learning_rate=best_params_neural_network[idx][0], dropout_rate=best_params_neural_network[idx][1], l2_lambda=best_params_neural_network[idx][2], batch_size=best_params_neural_network[idx][3], epochs=best_params_neural_network[idx][4])\n",
    "\n",
    "    XGBoostModel(X_train, y_train, X_test, y_test, max_depth=best_params_xgboost[idx][0], learning_rate=best_params_xgboost[idx][1], gamma=best_params_xgboost[idx][2], reg_lambda=best_params_xgboost[idx][3], n_estimators=best_params_xgboost[idx][4])\n",
    "\n",
    "    AdaBoostModel(X_train, y_train, X_test, y_test, learning_rate=best_params_adaboost[idx][0], max_depth=best_params_adaboost[idx][1], n_estimators=best_params_adaboost[idx][2])\n",
    "\n",
    "    KNNModel(X_train, y_train, X_test, y_test, k=best_params_knn[idx][0])\n",
    "\n",
    "    RandomForestModel(X_train, y_train, X_test, y_test, max_depth=best_params_random_forest[idx][0], n_estimators=best_params_random_forest[idx][1], min_samples_split=best_params_random_forest[idx][2])\n",
    "\n",
    "    print(\"\\n\\n\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
