{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Numerically stable sigmoid function.\"\"\"\n",
    "    z = np.clip(z, -500, 500)  # Limit z to avoid overflow\n",
    "    return np.where(z >= 0,\n",
    "                    1 / (1 + np.exp(-z)),\n",
    "                    np.exp(z) / (1 + np.exp(z)))\n",
    "\n",
    "def cross_entropy(y, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def compute_gradients(X, y, y_pred):\n",
    "    diff = y_pred - y\n",
    "    return np.dot(X.T, diff) / len(y)\n",
    "\n",
    "def initialize_weights(size):\n",
    "    std_dev = np.sqrt(2 / (size + 1))\n",
    "    return np.random.randn(size) * std_dev\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.weights = initialize_weights(X_train.shape[1])\n",
    "        self.losses = []\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = sigmoid(np.dot(X_train, self.weights))\n",
    "            loss = cross_entropy(y_train, y_pred)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            gradients = compute_gradients(X_train, y_train, y_pred)\n",
    "            self.weights -= self.lr * gradients\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        return sigmoid(np.dot(X, self.weights))\n",
    "        \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_probabilities(X) >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "class KNN:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = np.bincount(k_nearest_labels).argmax()\n",
    "        return most_common\n",
    "    \n",
    "    def predict_probabilities(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        probs = np.bincount(k_nearest_labels, minlength=np.max(self.y_train)+1) / self.k\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INCARCARE SETURI DE DATE DE ANTRENARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_rounds_data.csv')\n",
    "df = df.drop(columns=[f'player_{i}_{suffix}' for i in range(1, 11) for suffix in ['team_name', 'name']])\n",
    "X = df.drop(['round_winner'], axis=1)\n",
    "y = df['round_winner']\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "ds_name = \"rounds_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegression(lr=0.3, epochs=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Logistic Regression model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_neural_network(X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "    model.fit(X_train, y_train, batch_size=64, validation_data=(X_test, y_test), epochs=1000, callbacks=[es], verbose=0)\n",
    "    model.save('neural_network_model.h5')\n",
    "    print(\"Neural Network model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_xgboost(X_train, y_train):\n",
    "    model = XGBClassifier(max_depth=9, learning_rate=0.05, gamma=0.2, reg_lambda=0.2, n_estimators=146)\n",
    "    model.fit(X_train, y_train)\n",
    "    with open('xgboost_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"XGBoost model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_adaboost(X_train, y_train):\n",
    "    model = AdaBoostClassifier(algorithm=\"SAMME\", estimator=DecisionTreeClassifier(max_depth=9), learning_rate=0.1, n_estimators=22)\n",
    "    model.fit(X_train, y_train)\n",
    "    with open('adaboost_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"AdaBoost model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_knn(X_train, y_train):\n",
    "    model = KNN(k=45)\n",
    "    model.fit(X_train, y_train)\n",
    "    with open('knn_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"KNN model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_random_forest(X_train, y_train):\n",
    "    model = RandomForestClassifier(criterion=\"log_loss\", n_estimators=150, max_depth=40, min_samples_split=3, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    with open('random_forest_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Random Forest model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network model saved.\n",
      "XGBoost model saved.\n",
      "AdaBoost model saved.\n",
      "KNN model saved.\n",
      "Random Forest model saved.\n"
     ]
    }
   ],
   "source": [
    "train_and_save_logistic_regression(X_train, y_train)\n",
    "train_and_save_neural_network(X_train, y_train, X_test, y_test)\n",
    "train_and_save_xgboost(X_train, y_train)\n",
    "train_and_save_adaboost(X_train, y_train)\n",
    "train_and_save_knn(X_train, y_train)\n",
    "train_and_save_random_forest(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
